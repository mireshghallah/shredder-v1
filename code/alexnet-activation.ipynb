{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alexnet code from https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='/home/actlab/releq/data.imagenet' #'../../../ILSVRC'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 100 # originally 256 \n",
    "WORKERS = 1\n",
    "PRINT_FREQ = 5\n",
    "EPOCH = 40\n",
    "LR = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindir = os.path.join(DATA_DIR, 'train')\n",
    "valdir = os.path.join(DATA_DIR, 'val')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    traindir,\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=(None is None),\n",
    "        num_workers=WORKERS, pin_memory=True, sampler= None)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(valdir, transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = LR * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, model_original, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    SNR = []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(acc1[0], input.size(0))\n",
    "            top5.update(acc5[0], input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            #\n",
    "            output_original = model_original(input)\n",
    "            output = model.model_pt1(input)\n",
    "            output = model.intermed(output)\n",
    "\n",
    "            if i % PRINT_FREQ== 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                      'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                       i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                       top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg, sum(SNR)/len(SNR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, model_original,criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        input , target = input.to(device), target.to(device)\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(acc1[0], input.size(0))\n",
    "        top5.update(acc5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        output_original = model_original(input)\n",
    "        output_syn = model.model_pt1(input)\n",
    "        output_syn = model.intermed(output_syn)\n",
    "\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "            if (top1.val> 47.0):\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Conv layer numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_original = models.alexnet(pretrained= True)\n",
    "model_original.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "print(model_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = []\n",
    "fc_layers=[]\n",
    "for i, layer in enumerate(model_original.features):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        if ((i is not 0 )):\n",
    "            conv_layers.append(i)\n",
    "conv_layers.append(len(model_original.features))\n",
    "for i, layer in enumerate(model_original.classifier):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        fc_layers.append(i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract activation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_shapes=[]\n",
    "for cnt2, (data, target) in enumerate(val_loader):\n",
    "    for cnt,i in enumerate(conv_layers):\n",
    "        data,target = data.to(device), target.to(device)\n",
    "\n",
    "        newmodel_original =  torch.nn.Sequential(*(list(model_original.features)[0:i]))\n",
    "        \n",
    "        output_original = newmodel_original(data)\n",
    "        conv_shapes.append(output_original.shape[1:])\n",
    "   \n",
    "    if (cnt2==0):\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build new model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyActivation(nn.Module):\n",
    "    def __init__(self, activation_size):\n",
    "        super(NoisyActivation, self).__init__()\n",
    "        m =torch.distributions.laplace.Laplace(loc = 0.6, scale = 1.2, validate_args=None)\n",
    "        self.noise = nn.Parameter(m.rsample(activation_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        return input + self.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alexnet_syn(nn.Module):\n",
    "\n",
    "    def __init__(self, model_features, model_classifier, conv_layers, conv_shapes, index ):\n",
    "        super(alexnet_syn, self).__init__()\n",
    "        \n",
    "        self.model_pt1 =  torch.nn.Sequential(*(list(model_features)[0:conv_layers[index]]))\n",
    "        self.intermed = NoisyActivation(conv_shapes[index])\n",
    "        self.model_pt2 =  torch.nn.Sequential(*(list(model_features)[conv_layers[index]:]))\n",
    "        self.model_pt3 = model_classifier\n",
    "        for params in self.model_pt1.parameters():\n",
    "            params.requires_grad = False\n",
    "        for params in self.model_pt2.parameters():\n",
    "            params.requires_grad = False\n",
    "        for params in self.model_pt3.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.model_pt1(img)\n",
    "        x = self.intermed (x)\n",
    "        x = self.model_pt2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.model_pt3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.5)\n",
       "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): Dropout(p=0.5)\n",
       "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (5): ReLU(inplace)\n",
       "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_original = models.alexnet(pretrained= True)\n",
    "model_original.to(device)\n",
    "\n",
    "\n",
    "model_syn_original =torch.nn.Sequential(*(list(model_original.features)[0:conv_layers[4]]))\n",
    "model_syn_original_rest = torch.nn.Sequential(*(list(model_original.classifier)))\n",
    "\n",
    "model_syn_original.eval()\n",
    "model_syn_original_rest.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_syn = alexnet_syn(model_original.features, model_original.classifier, conv_layers, conv_shapes, 4)\n",
    "model_syn.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "weights_noise =np.expand_dims( model_syn.intermed.noise.cpu().detach().numpy(),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_syn.parameters()), lr=0.0001, weight_decay=-0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/12812]\tTime 1.296 (1.296)\tData 1.274 (1.274)\tLoss 2.8094 (2.8094)\tAcc@1 40.000 (40.000)\tAcc@5 66.000 (66.000)\n",
      "Epoch: [0][5/12812]\tTime 1.252 (1.288)\tData 1.234 (1.269)\tLoss 2.5751 (3.0683)\tAcc@1 44.000 (36.833)\tAcc@5 66.000 (60.000)\n",
      "Epoch: [0][10/12812]\tTime 1.577 (1.335)\tData 1.559 (1.316)\tLoss 3.1819 (3.0328)\tAcc@1 36.000 (37.818)\tAcc@5 61.000 (62.455)\n",
      "Epoch: [0][15/12812]\tTime 1.188 (1.296)\tData 1.169 (1.277)\tLoss 2.7690 (2.9808)\tAcc@1 44.000 (38.812)\tAcc@5 66.000 (63.000)\n",
      "Epoch: [0][20/12812]\tTime 1.138 (1.291)\tData 1.120 (1.272)\tLoss 2.8336 (2.9517)\tAcc@1 42.000 (38.810)\tAcc@5 70.000 (64.048)\n",
      "Epoch: [0][25/12812]\tTime 1.048 (1.281)\tData 1.029 (1.262)\tLoss 3.3255 (2.9539)\tAcc@1 31.000 (38.654)\tAcc@5 55.000 (64.192)\n",
      "Epoch: [0][30/12812]\tTime 1.107 (1.262)\tData 1.089 (1.243)\tLoss 2.7699 (2.9367)\tAcc@1 37.000 (38.645)\tAcc@5 64.000 (64.097)\n",
      "Epoch: [0][35/12812]\tTime 1.110 (1.266)\tData 1.091 (1.247)\tLoss 2.6138 (2.9279)\tAcc@1 46.000 (38.750)\tAcc@5 69.000 (64.111)\n",
      "Epoch: [0][40/12812]\tTime 1.280 (1.260)\tData 1.261 (1.241)\tLoss 2.6302 (2.9188)\tAcc@1 39.000 (38.951)\tAcc@5 64.000 (64.049)\n",
      "Epoch: [0][45/12812]\tTime 1.115 (1.248)\tData 1.097 (1.229)\tLoss 3.0663 (2.9092)\tAcc@1 40.000 (39.196)\tAcc@5 61.000 (64.109)\n",
      "Epoch: [0][50/12812]\tTime 1.354 (1.247)\tData 1.335 (1.228)\tLoss 2.7515 (2.9003)\tAcc@1 44.000 (39.314)\tAcc@5 64.000 (64.157)\n",
      "Epoch: [0][55/12812]\tTime 1.824 (1.261)\tData 1.806 (1.242)\tLoss 2.8458 (2.8972)\tAcc@1 38.000 (39.304)\tAcc@5 63.000 (64.268)\n",
      "Epoch: [0][60/12812]\tTime 1.091 (1.250)\tData 1.073 (1.231)\tLoss 3.0409 (2.8977)\tAcc@1 34.000 (39.246)\tAcc@5 62.000 (64.311)\n",
      "Epoch: [0][65/12812]\tTime 1.103 (1.246)\tData 1.085 (1.228)\tLoss 2.8899 (2.8945)\tAcc@1 39.000 (39.273)\tAcc@5 63.000 (64.348)\n",
      "Epoch: [0][70/12812]\tTime 1.079 (1.237)\tData 1.060 (1.218)\tLoss 2.9804 (2.8948)\tAcc@1 40.000 (39.225)\tAcc@5 60.000 (64.254)\n",
      "Epoch: [0][75/12812]\tTime 1.305 (1.234)\tData 1.287 (1.215)\tLoss 2.4327 (2.8868)\tAcc@1 48.000 (39.316)\tAcc@5 73.000 (64.250)\n",
      "Test: [0/500]\tTime 1.469 (1.469)\tLoss 1.4381 (1.4381)\tAcc@1 74.000 (74.000)\tAcc@5 84.000 (84.000)\n",
      "Test: [5/500]\tTime 1.257 (1.461)\tLoss 0.8368 (1.5167)\tAcc@1 79.000 (64.833)\tAcc@5 95.000 (86.000)\n",
      "Test: [10/500]\tTime 1.261 (1.399)\tLoss 2.0950 (1.4179)\tAcc@1 52.000 (67.091)\tAcc@5 79.000 (86.818)\n",
      "Test: [15/500]\tTime 1.251 (1.432)\tLoss 1.7626 (1.5262)\tAcc@1 59.000 (66.500)\tAcc@5 85.000 (85.312)\n",
      "Test: [20/500]\tTime 1.250 (1.428)\tLoss 2.1213 (1.7094)\tAcc@1 43.000 (61.810)\tAcc@5 80.000 (82.667)\n",
      "Test: [25/500]\tTime 1.299 (1.417)\tLoss 2.2930 (1.8670)\tAcc@1 57.000 (59.269)\tAcc@5 74.000 (80.269)\n",
      "Test: [30/500]\tTime 1.314 (1.407)\tLoss 3.0845 (2.0115)\tAcc@1 30.000 (55.903)\tAcc@5 63.000 (78.097)\n",
      "Test: [35/500]\tTime 1.243 (1.392)\tLoss 2.3771 (2.1335)\tAcc@1 58.000 (54.083)\tAcc@5 76.000 (76.250)\n",
      "Test: [40/500]\tTime 1.241 (1.403)\tLoss 1.9095 (2.1065)\tAcc@1 61.000 (54.098)\tAcc@5 80.000 (76.951)\n",
      "Test: [45/500]\tTime 1.375 (1.397)\tLoss 0.8086 (1.9879)\tAcc@1 81.000 (56.717)\tAcc@5 92.000 (78.543)\n",
      "Test: [50/500]\tTime 1.321 (1.387)\tLoss 1.6510 (1.9117)\tAcc@1 57.000 (58.431)\tAcc@5 88.000 (79.686)\n",
      "Test: [55/500]\tTime 1.132 (1.379)\tLoss 3.1989 (1.9523)\tAcc@1 47.000 (58.089)\tAcc@5 59.000 (78.964)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-811f7d498094>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0macc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSNR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_syn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_syn_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m54.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e4855fc221d8>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(val_loader, model, model_original, criterion)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-environments/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-environments/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMP_STATUS_CHECK_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model_syn,model_syn_original, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        acc1, SNR = validate(val_loader, model_syn, model_syn_original, criterion)\n",
    "        \n",
    "        if (acc1 > 54.5):\n",
    "            with open('activation-4-alexnet-test.csv','a') as fd:\n",
    "                writer = csv.writer(fd)\n",
    "                writer.writerow([SNR, acc, epoch])\n",
    "            weights_noise=np.concatenate((weights_noise,np.expand_dims(model_syn.intermed.noise.cpu().detach().numpy(),axis=0)),axis=0)\n",
    "            np.save(\"activations-4-noise-samples-alexnet-test\", weights_noise)\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
